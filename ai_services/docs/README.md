# AI Services æ¨¡å—ä½¿ç”¨æŒ‡å—

ä¸€ä¸ªç»Ÿä¸€çš„AIæœåŠ¡æ¨¡å—ï¼Œæä¾›Chatã€Embeddingå’ŒRerankåŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æä¾›å•†ï¼ˆOllamaã€æœ¬åœ°æ¨¡å‹ç­‰ï¼‰ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install requests numpy sentence-transformers transformers torch
```

### åŸºæœ¬ä½¿ç”¨

```python
from ai_services import AIServiceFactory

# åˆ›å»ºæœåŠ¡å·¥å‚
factory = AIServiceFactory.create_default()

# ä½¿ç”¨ChatæœåŠ¡
chat_service = factory.create_service("chat")
response = chat_service.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response.content)

# ä½¿ç”¨EmbeddingæœåŠ¡
embedding_service = factory.create_service("embedding")
embeddings = embedding_service.embed(["æ–‡æœ¬1", "æ–‡æœ¬2"])
print(f"åµŒå…¥ç»´åº¦: {len(embeddings.vectors[0])}")

# ä½¿ç”¨RerankæœåŠ¡
rerank_service = factory.create_service("rerank")
results = rerank_service.rerank("æŸ¥è¯¢æ–‡æœ¬", ["å€™é€‰æ–‡æ¡£1", "å€™é€‰æ–‡æ¡£2"])
print(f"æœ€ä½³åŒ¹é…: {results.results[0].document}")
```

### é…ç½®ç®¡ç†

AI Services æ”¯æŒçµæ´»çš„é…ç½®ç®¡ç†ï¼š

```bash
# 1. å¤åˆ¶é…ç½®æ¨¡æ¿
cp config.example.yaml config.yaml

# 2. éªŒè¯é…ç½®
python validate_config.py config.yaml

# 3. è¿è¡Œæµ‹è¯•
python quick_start.py
```

**é…ç½®æ–‡ä»¶è¯´æ˜ï¼š**
- `config.template.yaml` - å®Œæ•´é…ç½®æ¨¡æ¿ï¼ˆåŒ…å«æ‰€æœ‰é€‰é¡¹ï¼‰
- `config.example.yaml` - ç®€åŒ–é…ç½®ç¤ºä¾‹ï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰
- `CONFIG.md` - è¯¦ç»†é…ç½®æŒ‡å—

**é…ç½®éªŒè¯ï¼š**
```bash
# éªŒè¯æŒ‡å®šé…ç½®æ–‡ä»¶
python validate_config.py config.yaml --verbose

# éªŒè¯é»˜è®¤é…ç½®
python validate_config.py --default
```

## ğŸ“‹ ç›®å½•ç»“æ„

```
ai_services/
â”œâ”€â”€ __init__.py              # æ¨¡å—å…¥å£
â”œâ”€â”€ core/                    # æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ factory.py          # AIæœåŠ¡å·¥å‚
â”‚   â”œâ”€â”€ interfaces.py       # åŸºç¡€æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ exceptions.py       # å¼‚å¸¸ç±»å®šä¹‰
â”‚   â””â”€â”€ config_manager.py   # é…ç½®ç®¡ç†å™¨
â”œâ”€â”€ services/               # å…·ä½“æœåŠ¡å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py          # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ chat_service.py    # ChatæœåŠ¡
â”‚   â”œâ”€â”€ embedding_service.py # EmbeddingæœåŠ¡
â”‚   â””â”€â”€ rerank_service.py  # RerankæœåŠ¡
â”œâ”€â”€ config/                # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py         # é…ç½®å·¥å…·
â”œâ”€â”€ examples/             # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ tests/               # æµ‹è¯•ä»£ç 
â””â”€â”€ docs/               # æ–‡æ¡£
    â””â”€â”€ README.md       # æœ¬æ–‡æ¡£
```

## âš™ï¸ é…ç½®ç®¡ç†

### 1. ä½¿ç”¨é»˜è®¤é…ç½®

```python
from ai_services import AIServiceFactory

# ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºå·¥å‚
factory = AIServiceFactory.create_default()
```

### 2. ä»é…ç½®æ–‡ä»¶åŠ è½½

```python
# åˆ›å»ºé…ç½®æ¨¡æ¿
from ai_services.config import create_config_template
create_config_template("config.yaml", format="yaml")

# ä»é…ç½®æ–‡ä»¶åŠ è½½
factory = AIServiceFactory.from_config_file("config.yaml")
```

### 3. ä»å­—å…¸é…ç½®

```python
config = {
    "services": {
        "chat": {
            "default_provider": "ollama",
            "providers": {
                "ollama": {
                    "base_url": "http://localhost:11434",
                    "model_name": "llama2"
                }
            }
        }
    }
}

factory = AIServiceFactory.from_config(config)
```

### 4. ç¯å¢ƒå˜é‡é…ç½®

æ”¯æŒä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
export OLLAMA_BASE_URL="http://localhost:11434"
export OLLAMA_CHAT_MODEL="llama2"
export OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
export OLLAMA_RERANK_MODEL="llama2"
export LOG_LEVEL="INFO"
```

## ğŸ”§ æœåŠ¡è¯¦ç»†ä½¿ç”¨

### ChatæœåŠ¡

```python
from ai_services import create_chat_service
from ai_services.services.models import create_user_message, create_system_message

# åˆ›å»ºChatæœåŠ¡
chat_service = create_chat_service()

# å•è½®å¯¹è¯
response = chat_service.chat("ä½ å¥½")
print(response.content)

# å¤šè½®å¯¹è¯
messages = [
    create_system_message("ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"),
    create_user_message("è¯·ä»‹ç»ä¸€ä¸‹Python"),
    create_user_message("å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ")
]
response = chat_service.chat(messages)
print(response.content)

# å¼‚æ­¥å¯¹è¯
import asyncio

async def async_chat():
    response = await chat_service.chat_async("å¼‚æ­¥æ¶ˆæ¯")
    print(response.content)

asyncio.run(async_chat())

# æµå¼å¯¹è¯
for chunk in chat_service.chat_stream("æµå¼æ¶ˆæ¯"):
    print(chunk.content, end="", flush=True)
```

### EmbeddingæœåŠ¡

```python
from ai_services import create_embedding_service

# åˆ›å»ºEmbeddingæœåŠ¡
embedding_service = create_embedding_service()

# å•ä¸ªæ–‡æœ¬åµŒå…¥
result = embedding_service.embed("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬")
print(f"åµŒå…¥å‘é‡ç»´åº¦: {len(result.vectors[0])}")

# æ‰¹é‡æ–‡æœ¬åµŒå…¥
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
result = embedding_service.embed(texts)
print(f"å¤„ç†äº† {len(result.vectors)} ä¸ªæ–‡æœ¬")

# å¼‚æ­¥åµŒå…¥
async def async_embed():
    result = await embedding_service.embed_async(texts)
    return result

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = embedding_service.compute_similarity(
    result.vectors[0], 
    result.vectors[1]
)
print(f"ç›¸ä¼¼åº¦: {similarity}")
```

### RerankæœåŠ¡

```python
from ai_services import create_rerank_service

# åˆ›å»ºRerankæœåŠ¡
rerank_service = create_rerank_service()

# é‡æ’åºæ–‡æ¡£
query = "Pythonç¼–ç¨‹è¯­è¨€"
documents = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "Javaæ˜¯é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€", 
    "Pythonå…·æœ‰ç®€æ´çš„è¯­æ³•",
    "JavaScriptç”¨äºWebå¼€å‘"
]

result = rerank_service.rerank(query, documents)

# æŸ¥çœ‹æ’åºç»“æœ
for i, item in enumerate(result.results):
    print(f"{i+1}. åˆ†æ•°: {item.score:.3f} - {item.document}")

# å¼‚æ­¥é‡æ’åº
async def async_rerank():
    result = await rerank_service.rerank_async(query, documents)
    return result
```

## ğŸ¯ é«˜çº§åŠŸèƒ½

### 1. æœåŠ¡å¥åº·æ£€æŸ¥

```python
# æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
is_healthy = chat_service.health_check()
print(f"ChatæœåŠ¡çŠ¶æ€: {'æ­£å¸¸' if is_healthy else 'å¼‚å¸¸'}")

# æ£€æŸ¥æ‰€æœ‰æœåŠ¡
factory = AIServiceFactory.create_default()
status = factory.health_check()
for service_type, is_healthy in status.items():
    print(f"{service_type}: {'æ­£å¸¸' if is_healthy else 'å¼‚å¸¸'}")
```

### 2. è·å–å¯ç”¨æä¾›å•†

```python
# è·å–ChatæœåŠ¡çš„å¯ç”¨æä¾›å•†
providers = factory.get_available_providers("chat")
print(f"å¯ç”¨çš„Chatæä¾›å•†: {providers}")

# æµ‹è¯•æä¾›å•†è¿æ¥
connection_ok = factory.test_provider_connection("chat", "ollama")
print(f"Ollamaè¿æ¥çŠ¶æ€: {'æ­£å¸¸' if connection_ok else 'å¼‚å¸¸'}")
```

### 3. æ¨¡å‹ç®¡ç†ï¼ˆOllamaï¼‰

```python
# è·å–å¯ç”¨æ¨¡å‹
models = chat_service.get_available_models()
print(f"å¯ç”¨æ¨¡å‹: {models}")

# æ‹‰å–æ–°æ¨¡å‹
success = chat_service.pull_model("llama2:7b")
print(f"æ¨¡å‹æ‹‰å–: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
```

### 4. é”™è¯¯å¤„ç†

```python
from ai_services.core.exceptions import (
    AIServiceError, 
    ConnectionError, 
    ModelNotFoundError
)

try:
    response = chat_service.chat("æµ‹è¯•æ¶ˆæ¯")
except ConnectionError as e:
    print(f"è¿æ¥é”™è¯¯: {e}")
except ModelNotFoundError as e:
    print(f"æ¨¡å‹æœªæ‰¾åˆ°: {e}")
except AIServiceError as e:
    print(f"AIæœåŠ¡é”™è¯¯: {e}")
```

## ğŸ”Œ æ”¯æŒçš„æä¾›å•†

### ChatæœåŠ¡
- **Ollama**: æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
  - æ”¯æŒæµå¼å“åº”
  - æ”¯æŒå¤šç§å¼€æºæ¨¡å‹
  - æ”¯æŒè‡ªå®šä¹‰å‚æ•°

### EmbeddingæœåŠ¡
- **Ollama**: ä½¿ç”¨Ollamaçš„åµŒå…¥æ¨¡å‹
- **Local**: æœ¬åœ°sentence-transformersæ¨¡å‹
  - æ”¯æŒCPU/GPUè®¡ç®—
  - æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹

### RerankæœåŠ¡
- **Embedding-based**: åŸºäºåµŒå…¥å‘é‡çš„é‡æ’åº
- **Ollama**: ä½¿ç”¨Ollamaè¿›è¡Œç”Ÿæˆå¼é‡æ’åº
- **Cross-encoder**: äº¤å‰ç¼–ç å™¨æ¨¡å‹é‡æ’åº

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†

```python
# Embeddingæ‰¹å¤„ç†
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
result = embedding_service.embed(texts)  # è‡ªåŠ¨æ‰¹å¤„ç†

# Rerankæ‰¹å¤„ç†
documents = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
result = rerank_service.rerank("æŸ¥è¯¢", documents)  # è‡ªåŠ¨æ‰¹å¤„ç†
```

### 2. å¼‚æ­¥å¤„ç†

```python
import asyncio

async def process_multiple():
    tasks = [
        chat_service.chat_async("æ¶ˆæ¯1"),
        chat_service.chat_async("æ¶ˆæ¯2"),
        embedding_service.embed_async("æ–‡æœ¬1")
    ]
    results = await asyncio.gather(*tasks)
    return results
```

### 3. è¿æ¥æ± å’Œé‡è¯•

```python
# é…ç½®é‡è¯•å’Œè¶…æ—¶
config = {
    "services": {
        "chat": {
            "providers": {
                "ollama": {
                    "timeout": 60.0,      # è¶…æ—¶æ—¶é—´
                    "max_retries": 5,     # æœ€å¤§é‡è¯•æ¬¡æ•°
                    "base_url": "http://localhost:11434"
                }
            }
        }
    }
}
```

## ğŸ§ª æµ‹è¯•

```python
# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest tests/test_chat_service.py

# è¿è¡Œé›†æˆæµ‹è¯•
python -m pytest tests/test_integration.py
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Ollamaè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
   curl http://localhost:11434/api/tags
   
   # å¯åŠ¨Ollama
   ollama serve
   ```

2. **æ¨¡å‹æœªæ‰¾åˆ°**
   ```bash
   # æ‹‰å–æ‰€éœ€æ¨¡å‹
   ollama pull llama2
   ollama pull nomic-embed-text
   ```

3. **ä¾èµ–ç¼ºå¤±**
   ```bash
   # å®‰è£…æ‰€æœ‰ä¾èµ–
   pip install requests numpy sentence-transformers transformers torch
   ```

### æ—¥å¿—é…ç½®

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# æˆ–åœ¨é…ç½®ä¸­è®¾ç½®
config = {
    "logging": {
        "level": "DEBUG",
        "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    }
}
```

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒChatã€Embeddingã€RerankæœåŠ¡
- æ”¯æŒOllamaã€æœ¬åœ°æ¨¡å‹æä¾›å•†
- å®Œæ•´çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- å¼‚æ­¥å’Œæµå¼æ”¯æŒ

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªæ¨¡å—ï¼

## ğŸ“„ è®¸å¯è¯

MIT License